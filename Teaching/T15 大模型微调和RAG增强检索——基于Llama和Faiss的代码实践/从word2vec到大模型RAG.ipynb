{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2ca725-ae38-4fbc-b6ce-1e652050f946",
   "metadata": {},
   "source": [
    "# 词向量代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ffb354a2-f481-48c3-8698-77f654a506a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3ba93b67-3772-4962-8996-ca769f419af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch==2.4.0 in d:\\software\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in d:\\software\\anaconda3\\lib\\site-packages (from torch==2.4.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\software\\anaconda3\\lib\\site-packages (from torch==2.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\software\\anaconda3\\lib\\site-packages (from torch==2.4.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\software\\anaconda3\\lib\\site-packages (from torch==2.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\software\\anaconda3\\lib\\site-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\software\\anaconda3\\lib\\site-packages (from torch==2.4.0) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from jinja2->torch==2.4.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\software\\anaconda3\\lib\\site-packages (from sympy->torch==2.4.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af6d68-f4a9-4698-9c7d-ef085d295328",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d52fd8e-e5d5-4a1b-a1ee-41e03c9b1a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2284d8230b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2059b2d-0f13-4bab-88de-076e66259617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a1f5a6b8-6e3f-428e-88ec-1172f27ff2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(ngrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "647d6143-964b-446c-bd0f-1e168c51ee32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4666b21c-5600-4a35-ab97-b752fb7abfac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520.0139417648315\n",
      "517.8337724208832\n",
      "515.665666103363\n",
      "513.509486913681\n",
      "511.36414885520935\n",
      "509.2290771007538\n",
      "507.10294556617737\n",
      "504.98508381843567\n",
      "502.87503957748413\n",
      "500.7723731994629\n",
      "tensor([ 0.7493,  1.1847,  0.5673,  0.4106, -0.8592,  0.1352, -1.7765,  1.1096,\n",
      "         0.4932, -0.4825], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    print(total_loss)\n",
    "# print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00409224-0980-461c-a6fc-e9c30807d859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "    )\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75fb5c9-b718-4db8-93aa-e399d48391af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 34, 30, 29])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# Create your model and train. Here are some functions to help you make\n",
    "# the data ready for use by your module.\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067c528-2f84-4754-9159-29f3ca74cc78",
   "metadata": {},
   "source": [
    "# 句向量代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad95931-c696-414d-8705-3c145dd6dae0",
   "metadata": {},
   "source": [
    "https://github.com/cbowdon/doc2vec-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec537d71-b623-4067-bb49-b41079f5eb58",
   "metadata": {},
   "source": [
    "https://huggingface.co/spacy/en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "77e746fe-7070-4949-907a-0fe1d9d12bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: spacy==3.7.6 in d:\\software\\anaconda3\\lib\\site-packages (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (3.1.2)\n",
      "Requirement already satisfied: setuptools in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy==3.7.6) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\software\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.6) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.6) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.6) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\software\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.6) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\software\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.6) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.6) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\software\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy==3.7.6) (13.8.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\software\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.6) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\software\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy==3.7.6) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from jinja2->spacy==3.7.6) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in d:\\software\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.6) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\software\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\software\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.7.6) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==3.7.6  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ed650e2-1e5c-4dd3-82c1-825c9e86dc01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting en-core-web-sm==any\n",
      "  Downloading https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB 93.5 kB/s eta 0:02:17\n",
      "     ---------------------------------------- 0.0/12.8 MB 93.5 kB/s eta 0:02:17\n",
      "     --------------------------------------- 0.0/12.8 MB 109.3 kB/s eta 0:01:57\n",
      "     --------------------------------------- 0.1/12.8 MB 114.1 kB/s eta 0:01:52\n",
      "     --------------------------------------- 0.1/12.8 MB 114.1 kB/s eta 0:01:52\n",
      "     --------------------------------------- 0.1/12.8 MB 140.6 kB/s eta 0:01:31\n",
      "     --------------------------------------- 0.1/12.8 MB 174.7 kB/s eta 0:01:13\n",
      "     --------------------------------------- 0.1/12.8 MB 173.6 kB/s eta 0:01:14\n",
      "     --------------------------------------- 0.1/12.8 MB 189.6 kB/s eta 0:01:07\n",
      "     --------------------------------------- 0.1/12.8 MB 196.6 kB/s eta 0:01:05\n",
      "      -------------------------------------- 0.2/12.8 MB 238.5 kB/s eta 0:00:53\n",
      "      -------------------------------------- 0.2/12.8 MB 265.1 kB/s eta 0:00:48\n",
      "      -------------------------------------- 0.2/12.8 MB 288.4 kB/s eta 0:00:44\n",
      "      -------------------------------------- 0.3/12.8 MB 321.5 kB/s eta 0:00:39\n",
      "      -------------------------------------- 0.3/12.8 MB 345.7 kB/s eta 0:00:37\n",
      "     - ------------------------------------- 0.4/12.8 MB 384.3 kB/s eta 0:00:33\n",
      "     - ------------------------------------- 0.4/12.8 MB 419.1 kB/s eta 0:00:30\n",
      "     - ------------------------------------- 0.5/12.8 MB 471.0 kB/s eta 0:00:27\n",
      "     - ------------------------------------- 0.6/12.8 MB 528.1 kB/s eta 0:00:24\n",
      "     - ------------------------------------- 0.7/12.8 MB 581.7 kB/s eta 0:00:21\n",
      "     -- ------------------------------------ 0.7/12.8 MB 628.9 kB/s eta 0:00:20\n",
      "     -- ------------------------------------ 0.8/12.8 MB 698.8 kB/s eta 0:00:18\n",
      "     --- ----------------------------------- 1.0/12.8 MB 794.6 kB/s eta 0:00:15\n",
      "     --- ----------------------------------- 1.2/12.8 MB 892.2 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.3/12.8 MB 979.3 kB/s eta 0:00:12\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.7/12.8 MB 1.7 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.0/12.8 MB 1.9 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 3.0 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.1/12.8 MB 3.2 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.6/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.7/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.2/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.8/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.5/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.0/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in d:\\software\\anaconda3\\lib\\site-packages (from en-core-web-sm==any) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.1.2)\n",
      "Requirement already satisfied: setuptools in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\software\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in d:\\software\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\software\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\software\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\software\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (13.8.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\software\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\software\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in d:\\software\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\software\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\software\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.1.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22fee02-da10-47e6-a3c1-ac3cbc84d798",
   "metadata": {},
   "source": [
    "en_core_web_sm：English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.  \n",
    "加载了英语语言的核心模型，该模型包含了一系列的语言处理组件，如分词、词性标注、命名实体识别等。 它是SpaCy库中预训练好的一个小型模型，适用于一般的自然语言处理任务。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dbe31cb-d515-47b6-8454-3a67c92d3cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "\n",
    "example_df = pd.read_csv(\"doc2vec_example.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b066fbb-7651-4d9b-9363-9e61e4f914a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"By the half-light of a suspensor lamp, dimmed and hanging near the floor, the awakened boy could see a bulky female shape at his door, standing one step ahead of his mother. The old woman was a witch shadow - hair like matted spiderwebs, hooded 'round darkness of features, eyes like glittering jewels.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "example_df.iloc[i, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95043d6f-ca4a-46f9-8096-3ef829369cee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "By the half-light of a suspensor lamp, dimmed and hanging near the floor, the awakened boy could see a bulky female shape at his door, standing one step ahead of his mother. The old woman was a witch shadow - hair like matted spiderwebs, hooded 'round darkness of features, eyes like glittering jewels."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(example_df.iloc[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048ffd7e-169f-41a3-b8f1-25d0f1b8b565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp(example_df.iloc[i, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ceeaa43-8f4b-45ac-9053-9e82a60e25f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the week before their departure to Arrakis, when all the final scurrying about had reached a ...</td>\n",
       "      <td>[in, the, week, before, their, departure, to, arrakis, when, all, the, final, scurrying, about, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was a warm night at Castle Caladan, and the ancient pile of stone that had served the Atreide...</td>\n",
       "      <td>[it, was, a, warm, night, at, castle, caladan, and, the, ancient, pile, of, stone, that, had, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The old woman was let in by the side door down the vaulted passage by Paul's room and she was al...</td>\n",
       "      <td>[the, old, woman, was, let, in, by, the, side, door, down, the, vaulted, passage, by, paul, room...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By the half-light of a suspensor lamp, dimmed and hanging near the floor, the awakened boy could...</td>\n",
       "      <td>[by, the, half, light, of, a, suspensor, lamp, dimmed, and, hanging, near, the, floor, the, awak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  In the week before their departure to Arrakis, when all the final scurrying about had reached a ...   \n",
       "1  It was a warm night at Castle Caladan, and the ancient pile of stone that had served the Atreide...   \n",
       "2  The old woman was let in by the side door down the vaulted passage by Paul's room and she was al...   \n",
       "3  By the half-light of a suspensor lamp, dimmed and hanging near the floor, the awakened boy could...   \n",
       "\n",
       "                                                                                                tokens  \n",
       "0  [in, the, week, before, their, departure, to, arrakis, when, all, the, final, scurrying, about, ...  \n",
       "1  [it, was, a, warm, night, at, castle, caladan, and, the, ancient, pile, of, stone, that, had, se...  \n",
       "2  [the, old, woman, was, let, in, by, the, side, door, down, the, vaulted, passage, by, paul, room...  \n",
       "3  [by, the, half, light, of, a, suspensor, lamp, dimmed, and, hanging, near, the, floor, the, awak...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(df):\n",
    "    df[\"tokens\"] = df.text.str.lower().str.strip().apply(lambda x: [token.text.strip() for token in nlp(x) if token.text.isalnum()])\n",
    "    return df\n",
    "\n",
    "example_df = tokenize_text(example_df)\n",
    "\n",
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09406e04-9e05-4f50-b45c-1c434ef0e9e4",
   "metadata": {},
   "source": [
    "生成一个词袋，给每个词袋增加ID  \n",
    "We will need to construct a vocabulary so we can reference every word by an ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2afe9b-b48e-4d36-8894-d3a32553b66d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset comprises 4 documents and 106 unique words (over the limit of 1 occurrences)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, all_tokens, min_count=2):\n",
    "        self.min_count = min_count\n",
    "        self.freqs = {t:n for t, n in Counter(all_tokens).items() if n >= min_count}\n",
    "        self.words = sorted(self.freqs.keys())\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.words)}\n",
    "        \n",
    "vocab = Vocab([tok for tokens in example_df.tokens for tok in tokens], min_count=1)\n",
    "\n",
    "print(f\"Dataset comprises {len(example_df)} documents and {len(vocab.words)} unique words (over the limit of {vocab.min_count} occurrences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec036b-6f70-4317-85ce-73f2c5184742",
   "metadata": {},
   "source": [
    "有一些词出现的很少的话，对结果会有影响，所以可以删除掉，但是这里由于min_count为2，所以实际上这步操作无影响   \n",
    "Words that appear extremely rarely can harm performance, so we add a simple mechanism to strip those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560bad23-edc4-4e89-9891-6eca5287bbb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>length</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>clean_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the week before their departure to Arrakis, when all the final scurrying about had reached a ...</td>\n",
       "      <td>[in, the, week, before, their, departure, to, arrakis, when, all, the, final, scurrying, about, ...</td>\n",
       "      <td>32</td>\n",
       "      <td>[in, the, week, before, their, departure, to, arrakis, when, all, the, final, scurrying, about, ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was a warm night at Castle Caladan, and the ancient pile of stone that had served the Atreide...</td>\n",
       "      <td>[it, was, a, warm, night, at, castle, caladan, and, the, ancient, pile, of, stone, that, had, se...</td>\n",
       "      <td>39</td>\n",
       "      <td>[it, was, a, warm, night, at, castle, caladan, and, the, ancient, pile, of, stone, that, had, se...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The old woman was let in by the side door down the vaulted passage by Paul's room and she was al...</td>\n",
       "      <td>[the, old, woman, was, let, in, by, the, side, door, down, the, vaulted, passage, by, paul, room...</td>\n",
       "      <td>34</td>\n",
       "      <td>[the, old, woman, was, let, in, by, the, side, door, down, the, vaulted, passage, by, paul, room...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By the half-light of a suspensor lamp, dimmed and hanging near the floor, the awakened boy could...</td>\n",
       "      <td>[by, the, half, light, of, a, suspensor, lamp, dimmed, and, hanging, near, the, floor, the, awak...</td>\n",
       "      <td>53</td>\n",
       "      <td>[by, the, half, light, of, a, suspensor, lamp, dimmed, and, hanging, near, the, floor, the, awak...</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  text  \\\n",
       "0  In the week before their departure to Arrakis, when all the final scurrying about had reached a ...   \n",
       "1  It was a warm night at Castle Caladan, and the ancient pile of stone that had served the Atreide...   \n",
       "2  The old woman was let in by the side door down the vaulted passage by Paul's room and she was al...   \n",
       "3  By the half-light of a suspensor lamp, dimmed and hanging near the floor, the awakened boy could...   \n",
       "\n",
       "                                                                                                tokens  \\\n",
       "0  [in, the, week, before, their, departure, to, arrakis, when, all, the, final, scurrying, about, ...   \n",
       "1  [it, was, a, warm, night, at, castle, caladan, and, the, ancient, pile, of, stone, that, had, se...   \n",
       "2  [the, old, woman, was, let, in, by, the, side, door, down, the, vaulted, passage, by, paul, room...   \n",
       "3  [by, the, half, light, of, a, suspensor, lamp, dimmed, and, hanging, near, the, floor, the, awak...   \n",
       "\n",
       "   length  \\\n",
       "0      32   \n",
       "1      39   \n",
       "2      34   \n",
       "3      53   \n",
       "\n",
       "                                                                                          clean_tokens  \\\n",
       "0  [in, the, week, before, their, departure, to, arrakis, when, all, the, final, scurrying, about, ...   \n",
       "1  [it, was, a, warm, night, at, castle, caladan, and, the, ancient, pile, of, stone, that, had, se...   \n",
       "2  [the, old, woman, was, let, in, by, the, side, door, down, the, vaulted, passage, by, paul, room...   \n",
       "3  [by, the, half, light, of, a, suspensor, lamp, dimmed, and, hanging, near, the, floor, the, awak...   \n",
       "\n",
       "   clean_length  \n",
       "0            32  \n",
       "1            39  \n",
       "2            34  \n",
       "3            53  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_tokens(df, vocab):\n",
    "    df[\"length\"] = df.tokens.apply(len)\n",
    "    df[\"clean_tokens\"] = df.tokens.apply(lambda x: [t for t in x if t in vocab.freqs.keys()])\n",
    "    df[\"clean_length\"] = df.clean_tokens.apply(len)\n",
    "    return df\n",
    "\n",
    "example_df = clean_tokens(example_df, vocab)\n",
    "example_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b577f-e31b-4a62-bcca-6646d0f50bb2",
   "metadata": {},
   "source": [
    "The difficulty with our \"the cat _ on the mat\" problem is that the missing word could be any one in the vocabulary V and so the network would have |V| outputs for each input e.g. a huge vector containing zero for every word in the vocabulary and some positive number for \"sat\" if the network was perfectly trained. For calculating loss we need to turn that into a probabilty distribution, i.e. _softmax_ it. Computing the softmax for such a large vector is expensive.\n",
    "\n",
    "So the trick (one of many possible) we will use is _Noise Contrastive Estimation (NCE)_. We change our \"the cat _ on the mat\" problem into a multiple choice problem, asking the network to choose between \"sat\" and some random wrong answers like \"hopscotch\" and \"luxuriated\". This is easier to compute the softmax for since it's now a binary classifier (right or wrong answer) and the output is simply of a vector of size 1 + k where k is the number of random incorrect options.\n",
    "\n",
    "Happily, this alternative problem still learns equally useful word representations. We just need to adjust the examples and the loss function. There is a simplified version of the NCE loss function called _Negative Sampling (NEG)_ that we can use here.\n",
    "\n",
    "[Notes on Noise Contrastive Estimation and Negative Sampling (C. Dyer)](https://arxiv.org/abs/1410.8251) explains the derivation of the NCE and NEG loss functions.\n",
    "\n",
    "When we implement the loss function, we assume that the first element in a samples/scores vector is the score for the positive sample and the rest are negative samples. This convention saves us from having to pass around an auxiliary vector indicating which sample was positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c646ec-0a8f-4ba1-a681-23cd9c922bf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "ChatGPT翻译：\n",
    "m\n",
    "这段话主要解释了如何优化神经网络处理词填空问题时的计算效率，特别是在处理大词汇表的情况下。\n",
    "\n",
    "问题背景：\n",
    "在“the cat _ on the mat”这样的句子中，_ 处缺失的单词可能是词汇表 V 中的任何一个词。这意味着，神经网络需要为每个输入（句子）生成 |V| 个输出（即词汇表中每个词的概率），这是非常耗费计算资源的。为了得到这些输出的概率分布，通常需要对这些值进行 softmax 操作，但由于向量非常大，这一计算过程非常昂贵。\n",
    "\n",
    "解决方案：\n",
    "为了优化这个问题，作者提出了一种称为 噪声对比估计 (Noise Contrastive Estimation, NCE) 的方法。具体来说，这种方法将原来的词填空问题转换为一个多选问题。\n",
    "\n",
    "具体步骤：\n",
    "转换为多选问题：\n",
    "\n",
    "原本的问题是让网络在整个词汇表 V 中找到正确的词（比如“sat”）。\n",
    "现在的问题是让网络在正确答案（“sat”）和一些随机错误答案（比如“hopscotch”和“luxuriated”）之间选择正确的一个。\n",
    "计算优势：\n",
    "\n",
    "由于现在网络只需要在一个较小的选项集合中进行选择（正确答案 + 一些随机错误答案），而不再是整个词汇表，所以计算 softmax 的负担大大减轻。\n",
    "这个新的问题相当于一个二分类任务（正确或错误），输出向量的大小变成了 1 + k，其中 k 是随机错误选项的数量。\n",
    "学习效果：\n",
    "\n",
    "尽管问题形式发生了变化，网络仍然可以学习到同样有用的词表示。\n",
    "作者还提到了一种 NCE 损失函数的简化版本，称为 负采样 (Negative Sampling, NEG)，可以用于这个情境下。\n",
    "实现细节：\n",
    "\n",
    "在实现这个损失函数时，默认约定样本/得分向量中的第一个元素是正样本的得分，其他是负样本的得分。这样可以减少传递额外信息的需求，提高效率。\n",
    "总结：\n",
    "简而言之，这段话讨论了一种通过转换问题形式（从寻找正确单词变为多选）来减少计算负担的方法，同时仍然能够达到相似的学习效果。这种方法主要用于优化大词汇表下的神经网络训练。m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d0e2d-debd-419c-8eff-4119b4d58e16",
   "metadata": {},
   "source": [
    "负采样（Negative Sampling）损失的 PyTorch 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1120b977-fa9b-4d15-86a8-7e7da75ee3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NegativeSampling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NegativeSampling, self).__init__()\n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "    def forward(self, scores):\n",
    "        batch_size = scores.shape[0]\n",
    "        n_negative_samples = scores.shape[1] - 1   # TODO average or sum the negative samples? Summing seems to be correct by the paper\n",
    "        positive = self.log_sigmoid(scores[:,0])  # 取第1个，是正样本的预测概率\n",
    "        negatives = torch.sum(self.log_sigmoid(-scores[:,1:]), dim=1)  # 取后面3个\n",
    "        return -torch.sum(positive + negatives) / batch_size  # average for batch\n",
    "\n",
    "loss = NegativeSampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2154e20-d0ab-470e-a9ff-fa054d40be16",
   "metadata": {},
   "source": [
    "$$\\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3af7b9-3e0f-44a1-8ddb-a8f1529919f7",
   "metadata": {},
   "source": [
    "测试一下函数的输出结果  \n",
    "It's helpful to play with some values to reassure ourselves that this function does the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d40eedc0-e091-47e9-b63c-c8bff0237df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, -1, -1, -1]</td>\n",
       "      <td>tensor(1.2530)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.5, -1, -1, -1]</td>\n",
       "      <td>tensor(1.4139)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, -1, -1, -1]</td>\n",
       "      <td>tensor(1.6329)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>tensor(2.7726)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "      <td>tensor(3.3927)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "      <td>tensor(4.6329)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.5, 1, 1, 1]</td>\n",
       "      <td>tensor(4.4139)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>tensor(4.2530)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              scores            loss\n",
       "0    [1, -1, -1, -1]  tensor(1.2530)\n",
       "1  [0.5, -1, -1, -1]  tensor(1.4139)\n",
       "2    [0, -1, -1, -1]  tensor(1.6329)\n",
       "3       [0, 0, 0, 0]  tensor(2.7726)\n",
       "4       [0, 0, 0, 1]  tensor(3.3927)\n",
       "5       [0, 1, 1, 1]  tensor(4.6329)\n",
       "6     [0.5, 1, 1, 1]  tensor(4.4139)\n",
       "7       [1, 1, 1, 1]  tensor(4.2530)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# 第一行表示一个理想的情况：模型对正样本的得分较高（1），对负样本的得分较低（-1）。在这种情况下，损失函数的输出应该较小，因为模型已经很好地完成了任务。\n",
    "# 倒数第二行的第一列的值是 0.5，表示模型对正样本的预测得分为 0.5。这意味着模型对这个正样本的置信度不是很高，但仍然认为它可能是正确的。\n",
    "# 最后一行表示一个糟糕的情况：模型对正样本和负样本的得分相同，完全没有区分能力。损失函数在这种情况下应该输出一个较大的值，以迫使模型在训练过程中进行调整，降低负样本的得分。\n",
    "data = [[[1, -1, -1, -1]],  # this dummy data uses -1 to 1, but the real model is unconstrained \n",
    "        [[0.5, -1, -1, -1]],\n",
    "        [[0, -1, -1, -1]],\n",
    "        [[0, 0, 0, 0]],\n",
    "        [[0, 0, 0, 1]],\n",
    "        [[0, 1, 1, 1]],\n",
    "        [[0.5, 1, 1, 1]],\n",
    "        [[1, 1, 1, 1]]]\n",
    "\n",
    "loss_df = pd.DataFrame(data, columns=[\"scores\"])\n",
    "loss_df[\"loss\"] = loss_df.scores.apply(lambda x: loss(torch.FloatTensor([x])))\n",
    "\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5f3c0-9225-4d9a-b161-ac46983790aa",
   "metadata": {},
   "source": [
    "Higher scores for the positive sample (always the first element) reduce the loss but higher scores for the negative samples increase the loss. This looks like the right behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910242b-171c-484a-83c5-39f359db56a7",
   "metadata": {},
   "source": [
    "With that in the bag, let's look at creating training data. The general idea is to create a set of examples where each example has:\n",
    "\n",
    "- doc id\n",
    "- sample ids - a collection of the target token and some noise tokens\n",
    "- context ids - tokens before and after the target token\n",
    "\n",
    "e.g. If our context size was 2, the first example from the above dataset would be:\n",
    "\n",
    "```\n",
    "{\"doc_id\": 0,\n",
    " # 样本ID集合包含目标词\"week\"的ID和一些从词汇表中随机挑选出的噪声词的ID\n",
    " \"sample_ids\": [word2idx[x] for x in [\"week\", \"random-word-from-vocab\", \"random-word-from-vocab\"...],\n",
    " # 上下文ID集合包含目标词前后的词汇“in”、“the”、“before”和“their”的ID\n",
    " \"context_ids\": [word2idx[x] for x in [\"in\", \"the\", \"before\", \"their\"]]}\n",
    " ```\n",
    " \n",
    " The random words are chosen according to a probability distribution:\n",
    " 这些随机词是按照某种概率分布来选择的：使用的是一个单词出现频率的分布，然后将其提升到3/4次方，这种方法由T. Mikolov等人在《Distributed Representations of Words and Phrases and their Compositionality》中提出。\n",
    " > a unigram distribution raised to the 3/4rd power, as proposed by T. Mikolov et al. in Distributed Representations of Words and Phrases and their Compositionality\n",
    "\n",
    "这么做的效果是略微增加了罕见词汇的相对概率。从y=x^0.75的图像可以看出，曲线在低频部分（y轴较小的部分）略高于y=x，这意味着低频词的选择概率被适当提高了。\n",
    "This has the effect of slightly increasing the relative probability of rare words (look at the graph of `y=x^0.75` below and see how the lower end is raised above `y=x`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71694d5b-729b-4d76-9640-f45965b11771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: altair==5.4.1 in d:\\software\\anaconda3\\lib\\site-packages (5.4.1)\n",
      "Requirement already satisfied: jinja2 in d:\\software\\anaconda3\\lib\\site-packages (from altair==5.4.1) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\software\\anaconda3\\lib\\site-packages (from altair==5.4.1) (4.17.3)\n",
      "Requirement already satisfied: narwhals>=1.5.2 in d:\\software\\anaconda3\\lib\\site-packages (from altair==5.4.1) (1.6.0)\n",
      "Requirement already satisfied: packaging in d:\\software\\anaconda3\\lib\\site-packages (from altair==5.4.1) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\software\\anaconda3\\lib\\site-packages (from altair==5.4.1) (4.12.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair==5.4.1) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in d:\\software\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair==5.4.1) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from jinja2->altair==5.4.1) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install altair==5.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a588cdf4-3375-4ace-b0ce-e4805533128c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module jsonschema has no attribute exceptions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01malt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0.01\u001b[39m), np\u001b[38;5;241m.\u001b[39mpower(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0.01\u001b[39m), \u001b[38;5;241m0.75\u001b[39m)), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\altair\\__init__.py:650\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m():\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __all__\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvegalite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvegalite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjupyter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JupyterChart\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\altair\\vegalite\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ruff: noqa: F403\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\altair\\vegalite\\v5\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ruff: noqa: F401, F403\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datum\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vegalite_compilers\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\altair\\expr\\__init__.py:7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConstExpression, FunctionExpression\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvegalite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExprRef \u001b[38;5;28;01mas\u001b[39;00m _ExprRef\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\altair\\expr\\core.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Union\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeAlias\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaBase\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDatumType\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"An object to assist in building Vega-Lite Expressions.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\altair\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     SHORTHAND_KEYS,\n\u001b[0;32m      3\u001b[0m     display_traceback,\n\u001b[0;32m      4\u001b[0m     infer_encoding_types,\n\u001b[0;32m      5\u001b[0m     infer_vegalite_type_for_pandas,\n\u001b[0;32m      6\u001b[0m     parse_shorthand,\n\u001b[0;32m      7\u001b[0m     sanitize_narwhals_dataframe,\n\u001b[0;32m      8\u001b[0m     sanitize_pandas_dataframe,\n\u001b[0;32m      9\u001b[0m     update_nested,\n\u001b[0;32m     10\u001b[0m     use_signature,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AltairDeprecationWarning, deprecated, deprecated_warn\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spec_to_html\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\altair\\utils\\core.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnarwhals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependencies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_polars, is_pandas_dataframe\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnarwhals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntoDataFrame\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maltair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaBase, Undefined\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Protocol, runtime_checkable\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\altair\\utils\\schemapi.py:61\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Never, Self\n\u001b[1;32m---> 61\u001b[0m ValidationErrorList: TypeAlias \u001b[38;5;241m=\u001b[39m List[jsonschema\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mValidationError]\n\u001b[0;32m     62\u001b[0m GroupedValidationErrors: TypeAlias \u001b[38;5;241m=\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, ValidationErrorList]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# This URI is arbitrary and could be anything else. It just cannot be an empty\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# string as we need to reference the schema registered in\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# the referencing.Registry.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\software\\anaconda3\\Lib\\site-packages\\jsonschema\\__init__.py:71\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     62\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing jsonschema.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and will be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremoved in a future release. Instead, use the FORMAT_CHECKER \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ValidatorForFormat\u001b[38;5;241m.\u001b[39mFORMAT_CHECKER\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module jsonschema has no attribute exceptions"
     ]
    }
   ],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame(zip(np.arange(0,1,0.01), np.power(np.arange(0,1,0.01), 0.75)), columns=[\"x\", \"y\"])\n",
    "alt.Chart(data, title=\"x^0.75\").mark_line().encode(x=\"x\", y=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0db65e7c-77bb-45d3-bde3-c76b40d1b2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NoiseDistribution:\n",
    "    def __init__(self, vocab):\n",
    "        self.probs = np.array([vocab.freqs[w] for w in vocab.words])\n",
    "        self.probs = np.power(self.probs, 0.75)\n",
    "        self.probs /= np.sum(self.probs)\n",
    "    def sample(self, n):\n",
    "        \"Returns the indices of n words randomly sampled from the vocabulary.\"\n",
    "        return np.random.choice(a=self.probs.shape[0], size=n, p=self.probs)\n",
    "        \n",
    "noise = NoiseDistribution(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e06a8c-d7e2-4847-8027-2ed60d3675c9",
   "metadata": {},
   "source": [
    "开始产生训练样本，正样本永远放在第一个   \n",
    "With this distribution, we advance through the documents creating examples. Note that we are always putting the positive sample first in the samples vector, following the convention the loss function expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57650809-cde8-4a27-9b8e-efa83aac1c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def example_generator(df, context_size, noise, n_negative_samples, vocab):\n",
    "    for doc_id, doc in df.iterrows():\n",
    "        for i in range(context_size, len(doc.clean_tokens) - context_size):\n",
    "            positive_sample = vocab.word2idx[doc.clean_tokens[i]]\n",
    "            sample_ids = noise.sample(n_negative_samples).tolist()\n",
    "            # Fix a wee bug - ensure negative samples don't accidentally include the positive\n",
    "            sample_ids = [sample_id if sample_id != positive_sample else -1 for sample_id in sample_ids]\n",
    "            sample_ids.insert(0, positive_sample)                \n",
    "            context = doc.clean_tokens[i - context_size:i] + doc.clean_tokens[i + 1:i + context_size + 1]\n",
    "            context_ids = [vocab.word2idx[w] for w in context]\n",
    "            yield {\"doc_ids\": torch.tensor(doc_id),  # we use plural here because it will be batched\n",
    "                   \"sample_ids\": torch.tensor(sample_ids), \n",
    "                   \"context_ids\": torch.tensor(context_ids)}\n",
    "            \n",
    "# 上下文为5，负样本个数每次随机出5个\n",
    "examples = example_generator(example_df, context_size=5, noise=noise, n_negative_samples=5, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b92dc3c-29ab-463b-9b18-2b3e1c11a30b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object example_generator at 0x0000021E8FFC9E40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867afe3-a406-4663-8aaa-9265f0aef907",
   "metadata": {},
   "source": [
    "Now we package this up as a good old PyTorch dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1a64f4-8719-4b5a-8809-36ce97d0e455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NCEDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = list(examples)  # just naively evaluate the whole damn thing - suboptimal!\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index]\n",
    "    \n",
    "dataset = NCEDataset(examples)\n",
    "dataloader = DataLoader(dataset, batch_size=2, drop_last=True, shuffle=True)  # TODO bigger batch size when not dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c3007be-66dd-48a8-bd15-8aba0ac126fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdb8dc81-026c-4f9f-8fec-4c001a81669c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_ids': tensor(0),\n",
       "  'sample_ids': tensor([28, 57, 18, 38, 67, 92]),\n",
       "  'context_ids': tensor([ 52,  91, 101,  15,  92,  93,   9, 102,   4,  91])},\n",
       " {'doc_ids': tensor(0),\n",
       "  'sample_ids': tensor([93, 82, 39, 66, 52, 12]),\n",
       "  'context_ids': tensor([ 91, 101,  15,  92,  28,   9, 102,   4,  91,  37])},\n",
       " {'doc_ids': tensor(0),\n",
       "  'sample_ids': tensor([ 9,  0, 86, 85, 66, 27]),\n",
       "  'context_ids': tensor([101,  15,  92,  28,  93, 102,   4,  91,  37,  76])}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.examples[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d3830-979a-4cbb-839e-5352fc3e533c",
   "metadata": {},
   "source": [
    "后续要分割batch，所以这里给batch增加一些信息，方便DEBUG可读性  \n",
    "It's going to also be useful to have a way to convert batches back to a readable form for debugging, so we add a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6119b1ad-f59c-4ac0-950b-ad6d1c3757de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': tensor(3),\n",
       "  'context': 'of his mother the old ____ was a witch shadow hair',\n",
       "  'context_ids': tensor([ 66,  49,  62,  91,  67,  99,   0, 104,  79,  44]),\n",
       "  'samples': ['woman', 'weather', 'night', 'cooled', 'family', 'let'],\n",
       "  'sample_ids': tensor([105, 100,  65,  24,  33,  57])},\n",
       " {'doc_id': tensor(0),\n",
       "  'context': 'had reached a nearly unbearable ____ an old crone came to',\n",
       "  'context_ids': tensor([43, 73,  0, 64, 95,  6, 67, 26, 21, 93]),\n",
       "  'samples': ['frenzy', 'bore', 'side', 'hooded', 'hair', 'the'],\n",
       "  'sample_ids': tensor([40, 16, 82, 51, 44, 91])}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def describe_batch(batch, vocab):\n",
    "    results = []\n",
    "    for doc_id, context_ids, sample_ids in zip(batch[\"doc_ids\"], batch[\"context_ids\"], batch[\"sample_ids\"]):\n",
    "        context = [vocab.words[i] for i in context_ids]\n",
    "        context.insert(len(context_ids) // 2, \"____\")\n",
    "        samples = [vocab.words[i] for i in sample_ids]\n",
    "        result = {\"doc_id\": doc_id,\n",
    "                  \"context\": \" \".join(context), \n",
    "                  \"context_ids\": context_ids, \n",
    "                  \"samples\": samples, \n",
    "                  \"sample_ids\": sample_ids}\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "describe_batch(next(iter(dataloader)), vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc6146-e396-4b61-9dca-0ac6b14bbeaf",
   "metadata": {},
   "source": [
    "把文档和句子都纳入输入计算当中  \n",
    "Let's jump into creating the model itself. There isn't much to it - we multiply the input paragraph and word matrices by the output layer. Combining the paragraph and word matrices is done by summing here, but it could also be done by concatenating the inputs. The original paper actually found concatenation works better, perhaps because summing loses word order information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cdb3f5d-ebd9-4bdc-937a-1ab77e3b0043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DistributedMemory(nn.Module):\n",
    "    def __init__(self, vec_dim, n_docs, n_words):\n",
    "        super(DistributedMemory, self).__init__()\n",
    "        self.paragraph_matrix = nn.Parameter(torch.randn(n_docs, vec_dim))\n",
    "        self.word_matrix = nn.Parameter(torch.randn(n_words, vec_dim))\n",
    "        self.outputs = nn.Parameter(torch.zeros(vec_dim, n_words))\n",
    "    \n",
    "    def forward(self, doc_ids, context_ids, sample_ids):\n",
    "        #print(self.paragraph_matrix[doc_ids,:].shape)                                                                       \n",
    "        #print(torch.sum(self.word_matrix[context_ids,:], dim=1).shape)\n",
    "        # 将句子向量和词向量（上下文）加和后训练，最后两个向量都能够拿的到           # first add doc ids to context word ids to make the inputs                    \n",
    "        inputs = torch.add(self.paragraph_matrix[doc_ids,:],                   # (batch_size, vec_dim)\n",
    "                           torch.sum(self.word_matrix[context_ids,:], dim=1))  # (batch_size, 2x context, vec_dim) -> sum to (batch_size, vec_dim)\n",
    "                                                                               #\n",
    "        #print(inputs.shape)                                                   # select the subset of the output layer for the NCE test\n",
    "        outputs = self.outputs[:,sample_ids]                                   # (vec_dim, batch_size, n_negative_samples + 1)\n",
    "                                                                               #\n",
    "        return torch.bmm(inputs.unsqueeze(dim=1),                              # then multiply with some munging to make the tensor shapes line up \n",
    "                         outputs.permute(1, 0, 2)).squeeze()                   # -> (batch_size, n_negative_samples + 1)\n",
    "\n",
    "model = DistributedMemory(vec_dim=50,  # 每个句子的向量长度设置为50个\n",
    "                          n_docs=len(example_df),\n",
    "                          n_words=len(vocab.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6766bff7-e9ac-41e2-97a3-a1ac7c91842a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.paragraph_matrix[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "346b2324-22da-4c9a-8b7e-0422455f892b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_matrix[[ 4, 91, 37, 76,  1, 73,  0, 64, 95, 40]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b871f6-408a-449a-81cb-cea0f32cf8dd",
   "metadata": {},
   "source": [
    "简单试试  \n",
    "Let's take it for a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d4f12a-c864-4f85-8e80-144417965bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model.forward(**next(iter(dataloader)))\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff05f-d744-409c-b15e-356389b58fd0",
   "metadata": {},
   "source": [
    "开始循环训练  \n",
    "Oh yeah, the output layer was initialized with zeros. Time to bash out a standard issue PyTorch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "793b8aa6-5e43-4b2a-98c1-70d34852c41b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from torch.optim import Adam  # ilenic uses Adam, but gensim uses plain SGD\n",
    "import numpy as np\n",
    "\n",
    "def train(model, dataloader, epochs=40, lr=1e-3):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    training_losses = []\n",
    "    try:\n",
    "        for epoch in trange(epochs, desc=\"Epochs\"):\n",
    "            epoch_losses = []\n",
    "            for batch in dataloader:\n",
    "                model.zero_grad()\n",
    "                logits = model.forward(**batch)\n",
    "                batch_loss = loss(logits)\n",
    "                epoch_losses.append(batch_loss.item())\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            training_losses.append(np.mean(epoch_losses))\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Interrupted on epoch {epoch}!\")\n",
    "    finally:\n",
    "        return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d5532-23e4-4e92-b319-28907eeeed1a",
   "metadata": {},
   "source": [
    "Now we'll sanity check by overfitting the example data. Training loss should drop from untrained loss to something close to the minimum possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8f82127-7a18-4382-9341-1c0027fba6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.53it/s]\n"
     ]
    }
   ],
   "source": [
    "training_losses = train(model, dataloader, epochs=40, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b830ba4-917d-4afc-ace0-de01a10197fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pd\u001b[38;5;241m.\u001b[39mSeries(training_losses)\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.Series(training_losses).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bb6fad5-18d0-4c75-b7eb-e7075f601ace",
   "metadata": {
    "tags": []
   },
   "source": [
    "import altair as alt\n",
    "\n",
    "df_loss = pd.DataFrame(enumerate(training_losses), columns=[\"epoch\", \"training_loss\"])\n",
    "alt.Chart(df_loss).mark_bar().encode(alt.X(\"epoch\"), alt.Y(\"training_loss\", scale=alt.Scale(type=\"log\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10add4-df51-4ab1-b5e5-b5b73bfc65ef",
   "metadata": {},
   "source": [
    "因为我们有些多疑，让我们检查一下预测结果。   \n",
    "And because we're paranoid types, let's check a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3dd2beeb-7c51-43d3-8fb9-9ca75c131413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10.1291, -18.3573, -15.3337, -14.7885, -13.9782, -15.3337],\n",
       "        [ 15.4130, -21.7766, -19.5528, -19.1346, -17.1858, -17.8984]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model.forward(**next(iter(dataloader)))\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd924fdd-4ac8-44d3-a08a-7c75dedfeb30",
   "metadata": {},
   "source": [
    "正负样本能够区分的开，证明训练还不错  \n",
    "The positive sample gets a positive score and the negatives get negative scores. Super."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac50b71-5a90-4277-9c33-35356632285e",
   "metadata": {},
   "source": [
    "获取一下句向量的结果，然后看一下相似度  \n",
    "We should be able get the paragraph vectors for the documents and do things like check these for similarity to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5bfa13ff-0505-4ef4-9bef-1a74366f0659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6257e+00, -5.9801e-01, -2.0787e+00,  5.8892e-01,  3.0806e-01,\n",
       "          8.8510e-02, -1.1012e+00, -3.3149e+00,  1.5721e-01,  2.3342e+00,\n",
       "         -4.1842e-01,  7.7001e-01, -2.9533e-01,  6.4160e-01,  4.2092e-01,\n",
       "         -3.5608e-01, -2.4942e+00,  5.5928e-01, -1.9541e+00,  1.7930e+00,\n",
       "         -7.9386e-02,  3.7144e-01, -2.5290e+00,  5.2262e-01, -2.3597e+00,\n",
       "         -1.9081e+00, -5.9890e-01, -1.5461e+00,  7.3952e-01, -5.2003e-02,\n",
       "          7.9606e-01,  1.7315e+00,  5.0672e-01, -7.2812e-01,  1.1634e+00,\n",
       "         -1.1671e+00,  6.6750e-01,  5.8549e-01,  1.0763e+00,  9.2112e-01,\n",
       "          5.7500e-01, -2.2590e+00,  2.3657e+00,  5.2215e-01,  5.0826e-01,\n",
       "         -1.3147e+00,  1.1834e+00,  4.7461e-01,  2.4441e+00, -7.1212e-01],\n",
       "        [ 1.1969e+00,  4.8611e-01,  1.6409e+00, -2.2397e-01, -1.1347e+00,\n",
       "          1.3665e+00, -4.7226e-01, -1.1703e-01,  6.1717e-02,  4.9905e-01,\n",
       "         -6.6897e-01, -4.8154e-01, -8.4680e-01,  2.2461e-01,  1.6529e+00,\n",
       "         -9.9773e-01, -9.3588e-01,  2.3342e+00, -2.1419e-01, -3.2411e-01,\n",
       "         -3.3828e-01, -2.1945e-01, -1.7430e+00, -3.8314e-01, -4.2789e-01,\n",
       "          1.3508e+00,  1.6450e+00, -9.9306e-01,  6.8344e-01,  3.8330e-01,\n",
       "          2.5641e-01, -1.0884e+00,  1.9605e+00, -6.1263e-01, -4.1110e-01,\n",
       "         -3.4369e-01,  4.7663e-01,  1.2338e+00,  1.5074e-01,  1.0190e-01,\n",
       "         -1.3508e+00,  5.4379e-02,  1.2413e+00, -1.0745e-02,  1.5041e+00,\n",
       "         -1.1926e+00,  5.3594e-01,  9.4348e-01,  7.9038e-01, -3.8959e-01],\n",
       "        [ 1.9263e+00, -1.7335e-01,  2.0264e+00, -7.7119e-01, -5.7050e-01,\n",
       "          8.1883e-01,  1.0960e+00,  9.3478e-01, -1.1082e-01,  1.8621e+00,\n",
       "         -9.9479e-01,  8.1066e-01,  8.9917e-01,  2.3659e-01,  4.1176e-02,\n",
       "          1.4267e+00, -5.3208e-01,  1.0273e+00, -2.8195e-01, -2.0042e+00,\n",
       "          1.1525e+00,  1.0614e+00,  1.5522e-03,  3.2726e-01,  1.2638e-01,\n",
       "          2.3630e+00, -3.8029e+00, -3.2774e+00, -4.4350e-01,  1.3102e+00,\n",
       "          7.6262e-01,  4.2360e+00,  1.5162e+00,  1.2555e+00, -8.7526e-01,\n",
       "          3.0808e-01,  3.7459e-01, -4.9818e-01, -2.1649e+00,  7.7838e-01,\n",
       "         -2.8816e-01, -1.5288e-02, -2.7142e-01, -9.5105e-01, -7.5920e-01,\n",
       "          3.1199e-01, -1.0701e+00,  1.9427e-01, -4.0505e-01, -9.0018e-01],\n",
       "        [-2.2268e-01,  1.0403e+00,  5.2533e-01, -4.6570e-01,  6.0825e-01,\n",
       "         -1.7074e-01, -2.0188e+00, -1.7145e-01,  6.8648e-01,  9.1051e-01,\n",
       "         -1.1877e+00,  5.6850e-01, -1.6203e-01, -1.5207e+00,  5.8411e-01,\n",
       "         -4.5712e-01, -5.7461e-01,  1.1399e+00,  2.2963e-01,  3.6751e-01,\n",
       "          2.0862e+00,  1.9693e-01, -2.1550e-01, -7.7489e-01, -5.4175e-01,\n",
       "          1.7059e+00,  1.0578e+00,  1.2340e+00,  1.0494e+00,  2.1428e+00,\n",
       "          3.9282e-01, -8.5539e-01,  2.7395e+00,  3.8392e-01, -1.8168e+00,\n",
       "         -2.0588e-01, -3.5527e-01, -1.4116e-01,  7.6850e-01,  1.0020e-01,\n",
       "         -9.7428e-01,  3.5483e-01,  8.6379e-01, -1.1622e+00,  9.3356e-01,\n",
       "          8.1746e-01, -1.0962e+00, -8.0557e-01, -4.6662e-01,  1.0164e+00]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.paragraph_matrix.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "32a83e93-a00f-4ca0-b6b1-081a6485f7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.paragraph_matrix.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f35d5f89-0dff-493d-9e69-d786b827bbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>It was a warm night at Castle Caladan, and the ancient pile of stone that had served the Atreide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.386888</td>\n",
       "      <td>By the half-light of a suspensor lamp, dimmed and hanging near the floor, the awakened boy could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.274491</td>\n",
       "      <td>In the week before their departure to Arrakis, when all the final scurrying about had reached a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.066408</td>\n",
       "      <td>The old woman was let in by the side door down the vaulted passage by Paul's room and she was al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  similarity  \\\n",
       "1       1    1.000000   \n",
       "3       3    0.386888   \n",
       "0       0    0.274491   \n",
       "2       2    0.066408   \n",
       "\n",
       "                                                                                                  text  \n",
       "1  It was a warm night at Castle Caladan, and the ancient pile of stone that had served the Atreide...  \n",
       "3  By the half-light of a suspensor lamp, dimmed and hanging near the floor, the awakened boy could...  \n",
       "0  In the week before their departure to Arrakis, when all the final scurrying about had reached a ...  \n",
       "2  The old woman was let in by the side door down the vaulted passage by Paul's room and she was al...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def most_similar(paragraph_matrix, docs_df, index, n=None):\n",
    "    pm = normalize(paragraph_matrix, norm=\"l2\")  # in a smarter implementation we would cache this somewhere\n",
    "    sims = np.dot(pm, pm[index,:])\n",
    "    df = pd.DataFrame(enumerate(sims), columns=[\"doc_id\", \"similarity\"])\n",
    "    n = n if n is not None else len(sims)\n",
    "    return df.merge(docs_df[[\"text\"]].reset_index(drop=True), left_index=True, right_index=True).sort_values(by=\"similarity\", ascending=False)[:n]\n",
    "\n",
    "most_similar(model.paragraph_matrix.data, example_df, 1, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e31f6-363d-4c89-a90d-1a3c5bf70b06",
   "metadata": {},
   "source": [
    "不过，对于我们这小小的虚拟数据集来说，这并没有特别启发性的作用。我们还可以使用主成分分析（PCA）将我们的 n 维段落向量降维到 2 维，看看它们是否能够很好地聚类。  \n",
    "It's not particularly illuminating for our tiny set of dummy data though. We can also use PCA to reduce our n-dimensional paragraph vectors to 2 dimensions and see if they are clustered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3af831e9-1946-4411-a897-6a2d0c2e8af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-component PCA, explains 54.34% of variance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-9f6db6b3afce4cdf894ffb9b50cc518d.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-9f6db6b3afce4cdf894ffb9b50cc518d.vega-embed details,\n",
       "  #altair-viz-9f6db6b3afce4cdf894ffb9b50cc518d.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-9f6db6b3afce4cdf894ffb9b50cc518d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9f6db6b3afce4cdf894ffb9b50cc518d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9f6db6b3afce4cdf894ffb9b50cc518d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-d3b7be5ceb1b535d6419dafcf31f223e\"}, \"mark\": {\"type\": \"point\"}, \"encoding\": {\"color\": {\"field\": \"group\", \"type\": \"nominal\"}, \"x\": {\"field\": \"x\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"y\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-d3b7be5ceb1b535d6419dafcf31f223e\": [{\"x\": 6.701288337710279, \"y\": -3.8507057011670183, \"group\": \"0\"}, {\"x\": 1.0869599032487955, \"y\": 2.8906043873670617, \"group\": \"1\"}, {\"x\": -6.571933287280577, \"y\": -4.449626960432655, \"group\": \"2\"}, {\"x\": -1.2163149536784934, \"y\": 5.4097282742326085, \"group\": \"3\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_2d(paragraph_matrix, groups):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_dims = pca.fit_transform(paragraph_matrix)\n",
    "    print(f\"2-component PCA, explains {sum(pca.explained_variance_):.2f}% of variance\")\n",
    "    df = pd.DataFrame(reduced_dims, columns=[\"x\", \"y\"])\n",
    "    df[\"group\"] = groups\n",
    "    return df\n",
    "\n",
    "example_2d = pca_2d(model.paragraph_matrix.data, [\"0\",\"1\",\"2\",\"3\"])\n",
    "alt.Chart(example_2d).mark_point().encode(x=\"x\", y=\"y\", color=\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489900e-1899-4bf9-af1c-e2e23e8dc879",
   "metadata": {},
   "source": [
    "# Faiss匹配实现"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62fac6ed-f8a0-4061-b0ba-90569cc3d21a",
   "metadata": {},
   "source": [
    "qianfan==0.4.6\n",
    "langchain==0.2.12\n",
    "faiss-cpu==1.8.0\n",
    "langchain_community==0.2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c3cb5799-fd32-4585-8833-57b43001144b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: qianfan==0.4.6 in d:\\software\\anaconda3\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: aiohttp>=3.7.0 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (3.8.5)\n",
      "Requirement already satisfied: aiolimiter>=1.1.0 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (1.1.0)\n",
      "Requirement already satisfied: bce-python-sdk>=0.8.79 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (0.9.19)\n",
      "Requirement already satisfied: cachetools>=5.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (5.5.0)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (5.6.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (0.70.14)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.38 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (3.0.47)\n",
      "Requirement already satisfied: pydantic>=1.0 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (1.10.8)\n",
      "Requirement already satisfied: python-dotenv>=1.0 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (1.0.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.24 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (2.31.0)\n",
      "Requirement already satisfied: rich>=13.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (13.8.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (8.5.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\software\\anaconda3\\lib\\site-packages (from qianfan==0.4.6) (0.12.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.0->qianfan==0.4.6) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.0->qianfan==0.4.6) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.0->qianfan==0.4.6) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.0->qianfan==0.4.6) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.0->qianfan==0.4.6) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.0->qianfan==0.4.6) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.0->qianfan==0.4.6) (1.2.0)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in d:\\software\\anaconda3\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan==0.4.6) (3.20.0)\n",
      "Requirement already satisfied: future>=0.6.0 in d:\\software\\anaconda3\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan==0.4.6) (0.18.3)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from bce-python-sdk>=0.8.79->qianfan==0.4.6) (1.16.0)\n",
      "Requirement already satisfied: dill>=0.3.6 in d:\\software\\anaconda3\\lib\\site-packages (from multiprocess>=0.70.12->qianfan==0.4.6) (0.3.6)\n",
      "Requirement already satisfied: wcwidth in d:\\software\\anaconda3\\lib\\site-packages (from prompt-toolkit>=3.0.38->qianfan==0.4.6) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from pydantic>=1.0->qianfan==0.4.6) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests>=2.24->qianfan==0.4.6) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests>=2.24->qianfan==0.4.6) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests>=2.24->qianfan==0.4.6) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from rich>=13.0.0->qianfan==0.4.6) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\software\\anaconda3\\lib\\site-packages (from rich>=13.0.0->qianfan==0.4.6) (2.15.1)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from typer>=0.9.0->qianfan==0.4.6) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from typer>=0.9.0->qianfan==0.4.6) (1.5.4)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer>=0.9.0->qianfan==0.4.6) (0.4.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\software\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->qianfan==0.4.6) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install qianfan==0.4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0a2f6df6-b500-4e62-9abf-624d0842121c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain==0.2.12 in d:\\software\\anaconda3\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (3.8.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (0.2.37)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (0.1.108)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\software\\anaconda3\\lib\\site-packages (from langchain==0.2.12) (8.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.12) (1.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\software\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain==0.2.12) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\software\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain==0.2.12) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\software\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain==0.2.12) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\software\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.12) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\software\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.12) (3.10.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.2.12) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.2.12) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.2.12) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.12) (2.0.1)\n",
      "Requirement already satisfied: anyio in d:\\software\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12) (3.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\software\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12) (1.0.5)\n",
      "Requirement already satisfied: sniffio in d:\\software\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\software\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.12) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\software\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain==0.2.12) (2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c69e82a2-c3a5-447f-b981-a2b77b4d8041",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain_community==0.2.11 in d:\\software\\anaconda3\\lib\\site-packages (0.2.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (0.2.12)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (0.2.37)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (0.1.108)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (1.24.3)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\software\\anaconda3\\lib\\site-packages (from langchain_community==0.2.11) (8.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.11) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.11) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.11) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.11) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.11) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.11) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.2.11) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\software\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.11) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.11) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.12->langchain_community==0.2.11) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\software\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.12->langchain_community==0.2.11) (1.10.8)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\software\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community==0.2.11) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\software\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community==0.2.11) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\software\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain_community==0.2.11) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\software\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community==0.2.11) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\software\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community==0.2.11) (3.10.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community==0.2.11) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community==0.2.11) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community==0.2.11) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community==0.2.11) (2.0.1)\n",
      "Requirement already satisfied: anyio in d:\\software\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.2.11) (3.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\software\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.2.11) (1.0.5)\n",
      "Requirement already satisfied: sniffio in d:\\software\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.2.11) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\software\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community==0.2.11) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\software\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain_community==0.2.11) (2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.2.11) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community==0.2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "42d0e572-ead8-4ecb-b514-afd3192503a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: faiss-cpu==1.8.0 in d:\\software\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in d:\\software\\anaconda3\\lib\\site-packages (from faiss-cpu==1.8.0) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "116d6242-3246-405a-8d74-6cdb9be93752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qianfan_ak = '你的AK'\n",
    "qianfan_sk = '你的SK'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d07d1b-8dcc-4ed8-b3d6-9c02e789aeb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "用的是智源研究院的embedding模型：https://huggingface.co/BAAI/bge-large-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3a10b7bf-956a-4644-967e-4dd4dca54d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings.baidu_qianfan_endpoint import QianfanEmbeddingsEndpoint\n",
    "\n",
    "str_docs = [\n",
    "    # 句子1，句意有农业保险\n",
    "    '实施金融支持农业纾难解困政策。鼓励市县积极开展特色渔业养殖保险，在参保农户自缴保费比例不低于20%的前提下，对险种绩效评价结果达标的市县，省级财政按'\n",
    "    '照25%的比例给予保费补贴。对2022年8月至12月到期的农民小额贷款和新发生的农民小额贷款贴息由5%提升至6%。对脱贫人口小额信贷，允许其调整还本计划或办'\n",
    "    '理贷款展期、续贷。对受疫情影响暂时出现还贷困难的涉农企业及农户(包括脱贫户、监测户)，支持银行机构按市场化原则予以降息、减息或免息扶持，开展征信保护等。',\n",
    "    # 句子2，句意无农业保险\n",
    "    '分级分类开展社会化服务。针对中高风险区农业生产人员无法外出生产问题，组织有关企业和社会化服务组织提供托管、代耕、代收服务。各村委会统计当地需要种植'\n",
    "    '或收获的作物品种、面积、产量，乡镇政府商请当地农业农村局统一协调专业化服务组织提供托管服务。当地力量不足时，市县农业农村局向省农业农村厅申请统一协调安排',\n",
    "    # 句子3，句子中有农业保险\n",
    "    '财政支持。各级财政部门履行牵头主责，会同有关部门从发展方向、制度设计、政策制定、资金保障等方面推进农业保险发展，通过保费补贴、机构遴选等多种政策手'\n",
    "    '段，发挥农业保险机制性工具作用，督促承保机构依法合规展业，充分调动各参与方积极性，推动农业保险高质量发展。'\n",
    "]\n",
    "doc_docs = [Document(i) for i in str_docs]\n",
    "\n",
    "embedding_model = QianfanEmbeddingsEndpoint(qianfan_ak=qianfan_ak, qianfan_sk=qianfan_sk, model='bge-large-zh')\n",
    "db = FAISS.from_documents(doc_docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7dccd252-d36b-422b-87f8-b6b33890d886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QianfanEmbeddingsEndpoint(qianfan_ak=None, qianfan_sk=None, chunk_size=16, model='bge-large-zh', endpoint='', client=<qianfan.resources.llm.embedding.Embedding object at 0x000002288A022C10>, init_kwargs={}, model_kwargs={})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ea83b-2359-44fb-b975-ed69b2a55f63",
   "metadata": {},
   "source": [
    "faiss.IndexFlatL2 是一个用于执行 L2 距离（欧几里得距离）计算的简单索引，但它不提供直接获取已添加嵌入向量的功能。每次添加向量到 FAISS 索引时，都应同时将这些向量保存在你能访问的地方。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "98499ba6-9af1-403e-aad7-7af759cd39ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x0000022889FC3DB0> >"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5f8ad825-eefe-4d7d-b1c7-d79b50d965aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bbf9ead0-3b68-409c-bb17-a5135552f644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 通过嵌入模型获取文本的嵌入向量\n",
    "embeddings = embedding_model.embed_documents(str_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "61236fbd-4f5a-466d-b0f8-de8229e53771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8b8cd2d3-ff22-4fb6-953e-b384e48a2c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "812e437a-89fc-4f6f-959c-14d6ce8d0147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_path = 'qianfan.db'\n",
    "# faiss是索引，pkl是文档\n",
    "db.save_local(db_path)  # 存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "742764cc-1354-4229-a4c9-b1d64a5036a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# db = FAISS.load_local(db_path, embedding, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7c740cc1-fad0-4ed5-a37b-7973d0efbd65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='财政支持。各级财政部门履行牵头主责，会同有关部门从发展方向、制度设计、政策制定、资金保障等方面推进农业保险发展，通过保费补贴、机构遴选等多种政策手段，发挥农业保险机制性工具作用，督促承保机构依法合规展业，充分调动各参与方积极性，推动农业保险高质量发展。' \n",
      " 0.34435654\n",
      "page_content='实施金融支持农业纾难解困政策。鼓励市县积极开展特色渔业养殖保险，在参保农户自缴保费比例不低于20%的前提下，对险种绩效评价结果达标的市县，省级财政按照25%的比例给予保费补贴。对2022年8月至12月到期的农民小额贷款和新发生的农民小额贷款贴息由5%提升至6%。对脱贫人口小额信贷，允许其调整还本计划或办理贷款展期、续贷。对受疫情影响暂时出现还贷困难的涉农企业及农户(包括脱贫户、监测户)，支持银行机构按市场化原则予以降息、减息或免息扶持，开展征信保护等。' \n",
      " 0.4557104\n",
      "page_content='分级分类开展社会化服务。针对中高风险区农业生产人员无法外出生产问题，组织有关企业和社会化服务组织提供托管、代耕、代收服务。各村委会统计当地需要种植或收获的作物品种、面积、产量，乡镇政府商请当地农业农村局统一协调专业化服务组织提供托管服务。当地力量不足时，市县农业农村局向省农业农村厅申请统一协调安排' \n",
      " 0.520805\n"
     ]
    }
   ],
   "source": [
    "query = \"农业保险\"\n",
    "docs_and_scores = db.similarity_search_with_score(query)  # 返回分数和内容\n",
    "\n",
    "for i, j in docs_and_scores:\n",
    "    print(i, '\\n', j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
