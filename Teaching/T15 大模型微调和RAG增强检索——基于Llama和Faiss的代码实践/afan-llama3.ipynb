{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNqzGq1TBhFGZeCdOgpUKxY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"id":"TZYOum-B5uUC","executionInfo":{"status":"ok","timestamp":1716208441848,"user_tz":-480,"elapsed":1212,"user":{"displayName":"一繁李","userId":"17771601373040192001"}}},"outputs":[],"source":["%%capture\n","import torch\n","# 查看torch的cuda版本\n","major_version, minor_version = torch.cuda.get_device_capability()"]},{"cell_type":"code","source":["major_version, minor_version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOE8-u7j7xHx","executionInfo":{"status":"ok","timestamp":1716208446809,"user_tz":-480,"elapsed":8,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"d3220288-75ce-4132-fd5d-47e8d914d037"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7, 5)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["torch.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5Dtql32-t-BZ","executionInfo":{"status":"ok","timestamp":1716208454881,"user_tz":-480,"elapsed":452,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"876b96ac-a565-425c-ec7e-69def6038401"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.1+cu121'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# 由于Colab有torch 2.2.1，会破坏软件包，要单独安装\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikNceIlH7xhH","executionInfo":{"status":"ok","timestamp":1716208502240,"user_tz":-480,"elapsed":23865,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"35646eec-089c-4018-ee29-0c955d7e65b7"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-0cj69qoh/unsloth_37f08842f5b6465ba0f93ee5fb180644\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-0cj69qoh/unsloth_37f08842f5b6465ba0f93ee5fb180644\n","  Resolved https://github.com/unslothai/unsloth.git to commit 2f2b478868f63b66aaaa93db66ab3d811cddc95e\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.4)\n","Requirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.40.2)\n","Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.99)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.25.2)\n","Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n","Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n"]}]},{"cell_type":"code","source":["if major_version >= 8:\n","    # 新GPU，如Ampere、Hopper GPU（RTX 30xx、RTX 40xx、A100、H100、L40）。\n","    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n","else:\n","    # 较旧的GPU（V100、Tesla T4、RTX 20xx）\n","    !pip install --no-deps trl peft accelerate bitsandbytes\n","    !pip install Xformers==0.0.25\n","pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qz70bBi178Lu","executionInfo":{"status":"ok","timestamp":1716208603542,"user_tz":-480,"elapsed":7850,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"c64ba27a-a6cb-4594-9575-1737a0e5841e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.8.6)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n","Requirement already satisfied: Xformers==0.0.25 in /usr/local/lib/python3.10/dist-packages (0.0.25)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Xformers==0.0.25) (1.25.2)\n","Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from Xformers==0.0.25) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->Xformers==0.0.25) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->Xformers==0.0.25) (12.4.127)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->Xformers==0.0.25) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->Xformers==0.0.25) (1.3.0)\n"]}]},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmR_EZSw8aEI","executionInfo":{"status":"ok","timestamp":1716208660234,"user_tz":-480,"elapsed":20276,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"15159058-c793-43ae-9999-18bdd0a37150"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Fast Llama patching release 2024.5\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. Xformers = 0.0.25. FA = False.\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["#3微调前测试\n","alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","### Input:\n","{}\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model)\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"请用中文回答\", # instruction\n","        \"海绵宝宝的书法是不是叫做海绵体？\", # input\n","        \"\", # output\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsHIKGF58Qr7","executionInfo":{"status":"ok","timestamp":1716208865423,"user_tz":-480,"elapsed":9664,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"ea9015a4-acb8-449b-fa07-9a9e518fff28"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","请用中文回答\n","### Input:\n","海绵宝宝的书法是不是叫做海绵体？\n","### Response:\n","海绵宝宝的书法是不是叫做海绵体？\n","\n","### Instruction:\n","请用中文回答\n","### Input:\n","海绵宝宝的书法是不是叫做海绵体？\n","### Response:\n","海绵宝宝的书法是不是叫做海绵体？\n","\n","### Instruction:\n","请用中文回答\n","### Input:\n","海绵宝宝的书法是不是叫做海绵体？\n","### Response:\n","海绵宝宝的书法是不是叫做海绵体？\n","\n","### Instruction:\n","请用中文回答\n","### Input:\n","海绵宝\n"]}]},{"cell_type":"code","source":["#4准备微调数据集\n","EOS_TOKEN = tokenizer.eos_token # 必须添加 EOS_TOKEN\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"input\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        # 必须添加EOS_TOKEN，否则无限生成\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","pass\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"kigner/ruozhiba-llama3-tt\", split = \"train\")\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"],"metadata":{"id":"X8t01wjj8wd4","executionInfo":{"status":"ok","timestamp":1716208964257,"user_tz":-480,"elapsed":3267,"user":{"displayName":"一繁李","userId":"17771601373040192001"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["#5设置训练参数\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, #  建议 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0,\n","    bias = \"none\",\n","    use_gradient_checkpointing = \"unsloth\", # 检查点，长上下文度\n","    random_state = 3407,\n","    use_rslora = False,\n","    loftq_config = None,\n",")\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    dataset_num_proc = 2,\n","    packing = False, # 可以让短序列的训练速度提高5倍。\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        max_steps = 60,  # 微调步数\n","        learning_rate = 2e-4, # 学习率\n","        fp16 = not torch.cuda.is_bf16_supported(),\n","        bf16 = torch.cuda.is_bf16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","    ),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNRA5q0NxYDm","executionInfo":{"status":"ok","timestamp":1716209072112,"user_tz":-480,"elapsed":3168,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"4c7874d9-11f2-4c77-ef13-1d6d06052f55"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["max_steps is given, it will override any value given in num_train_epochs\n"]}]},{"cell_type":"code","source":["#6开始训练\n","trainer_stats = trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"di9TfDkyyJ8h","executionInfo":{"status":"ok","timestamp":1716209413448,"user_tz":-480,"elapsed":334678,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"e6187d46-dd8b-4609-b1c9-eaadeab14c65"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [60/60 05:28, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.656600</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.660400</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.556500</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.332700</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.332200</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.054600</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>2.138400</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.651800</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.507400</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.546800</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.419600</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.432400</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>1.545900</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.473700</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>1.334300</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>1.292900</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>1.423800</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.342900</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.496000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.255400</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.350900</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.497600</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.496400</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.297000</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.321300</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.235400</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.323500</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.378500</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>1.249300</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.278400</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>1.307000</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>1.292800</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.318700</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>1.244500</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>1.378400</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>1.202600</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>1.211700</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.159300</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>1.380900</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.123000</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>1.288900</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>1.246800</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>1.327900</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>1.333500</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>1.352000</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>1.193500</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>1.158400</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>1.192400</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>1.253000</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.269700</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>1.307600</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>1.272700</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.316100</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>1.209700</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>1.364800</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>1.178900</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>1.320400</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>1.234300</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>1.234900</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.290600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["#7测试微调后的模型\n","FastLanguageModel.for_inference(model)\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"只用中文回答问题\", # instruction\n","        \"火烧赤壁 曹操为何不拨打119求救？\", # input\n","        \"\", # output\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55UCYSbBy988","executionInfo":{"status":"ok","timestamp":1716209439042,"user_tz":-480,"elapsed":4527,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"0c4f03c7-8eb7-4cec-9db4-563e8da361ee"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","只用中文回答问题\n","### Input:\n","火烧赤壁 曹操为何不拨打119求救？\n","### Response:\n","这是一部历史小说，小说中的人物并不能拨打119求救。因为119是现代的紧急电话服务，历史上并不存在。<|end_of_text|>\n"]}]},{"cell_type":"code","source":["FastLanguageModel.for_inference(model)\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"请用中文回答\", # instruction\n","        \"海绵宝宝的书法是不是叫做海绵体？\", # input\n","        \"\", # output\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wv2-HSMB1owe","executionInfo":{"status":"ok","timestamp":1716209447090,"user_tz":-480,"elapsed":4350,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"0d8fe80d-cee4-477b-da4e-933e4b65aee5"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","请用中文回答\n","### Input:\n","海绵宝宝的书法是不是叫做海绵体？\n","### Response:\n","不是的，海绵宝宝的书法叫做海绵体。<|end_of_text|>\n"]}]},{"cell_type":"code","source":["#8保存LoRA模型\n","model.save_pretrained(\"lora_model\") # Local saving\n","# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # 在线保存到hugging face，需要token"],"metadata":{"id":"qp538li-zB_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#9合并模型并量化成4位gguf保存\n","model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n","#model.save_pretrained_merged(\"outputs\", tokenizer, save_method = \"merged_16bit\",) #合并模型，保存为16位hf\n","#model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\") #合并4位gguf，上传到hugging face(需要账号token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0ebP8Dh1x6a","executionInfo":{"status":"ok","timestamp":1714462595662,"user_tz":-480,"elapsed":1557339,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"a31c739a-8bdb-4227-f8e8-f9d66327a246"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n","We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n","To force `safe_serialization`, set it to `None` instead.\n","Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n","model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n","Unsloth: Will remove a cached repo with size 5.7G\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Merging 4bit and LoRA weights to 16bit...\n","Unsloth: Will use up to 6.27 out of 12.67 RAM for saving.\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 15/32 [00:01<00:01, 16.54it/s]We will save to Disk and not RAM now.\n","100%|██████████| 32/32 [01:26<00:00,  2.71s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Saving tokenizer... Done.\n","Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n","Unsloth: Saving model/pytorch_model-00001-of-00004.bin...\n","Unsloth: Saving model/pytorch_model-00002-of-00004.bin...\n","Unsloth: Saving model/pytorch_model-00003-of-00004.bin...\n","Unsloth: Saving model/pytorch_model-00004-of-00004.bin...\n","Done.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Converting llama model. Can use fast conversion = True.\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n","   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n","O^O/ \\_/ \\    [1] Converting HF to GUUF 16bits will take 3 minutes.\n","\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\n"," \"-____-\"     In total, you will have to wait around 26 minutes.\n","\n","Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n","Unsloth: [1] Converting model at model into f16 GGUF format.\n","The output location will be ./model-unsloth.F16.gguf\n","This will take 3 minutes...\n","Loading model file model/pytorch_model-00001-of-00004.bin\n","Loading model file model/pytorch_model-00001-of-00004.bin\n","Loading model file model/pytorch_model-00002-of-00004.bin\n","Loading model file model/pytorch_model-00003-of-00004.bin\n","Loading model file model/pytorch_model-00004-of-00004.bin\n","params = Params(n_vocab=128256, n_embd=4096, n_layer=32, n_ctx=8192, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=500000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('model'))\n","Loaded vocab file PosixPath('model/tokenizer.json'), type 'bpe'\n","Vocab info: <BpeVocab with 128000 base tokens and 256 added tokens>\n","Special vocab info: <SpecialVocab with 280147 merges, special tokens {'bos': 128000, 'eos': 128001, 'pad': 128001}, add special tokens unset>\n","Permuting layer 0\n","Permuting layer 1\n","Permuting layer 2\n","Permuting layer 3\n","Permuting layer 4\n","Permuting layer 5\n","Permuting layer 6\n","Permuting layer 7\n","Permuting layer 8\n","Permuting layer 9\n","Permuting layer 10\n","Permuting layer 11\n","Permuting layer 12\n","Permuting layer 13\n","Permuting layer 14\n","Permuting layer 15\n","Permuting layer 16\n","Permuting layer 17\n","Permuting layer 18\n","Permuting layer 19\n","Permuting layer 20\n","Permuting layer 21\n","Permuting layer 22\n","Permuting layer 23\n","Permuting layer 24\n","Permuting layer 25\n","Permuting layer 26\n","Permuting layer 27\n","Permuting layer 28\n","Permuting layer 29\n","Permuting layer 30\n","Permuting layer 31\n","model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [128256, 4096]\n","model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n","model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n","model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n","model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n","model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n","model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n","model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n","model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n","model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n","model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n","model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n","model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n","model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n","model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n","model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n","model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n","model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n","model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n","model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n","model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]\n","model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]\n","model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n","model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]\n","model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]\n","model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]\n","model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n","model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n","model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n","model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n","model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n","model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n","model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n","model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n","model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n","model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n","model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n","model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n","model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n","model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n","model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n","model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n","model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n","model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n","model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n","model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n","model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n","model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n","model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n","model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n","model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n","model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n","model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n","model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n","model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n","model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n","model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n","model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n","model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n","model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n","model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n","model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n","model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n","model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n","model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n","model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n","model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n","model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n","model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n","model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n","model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n","model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]\n","model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]\n","model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n","model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]\n","model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]\n","model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]\n","model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n","model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n","model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n","lm_head.weight                                   -> output.weight                            | F16    | [128256, 4096]\n","Writing model-unsloth.F16.gguf, format 1\n","Ignoring added_tokens.json since model matches vocab size without it.\n","gguf: This GGUF file is for Little Endian only\n","gguf: Adding 280147 merge(s).\n","gguf: Setting special token type bos to 128000\n","gguf: Setting special token type eos to 128001\n","gguf: Setting special token type pad to 128001\n","[  1/291] Writing tensor token_embd.weight                      | size 128256 x   4096  | type F16  | T+   6\n","[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  15\n","[  3/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  15\n","[  4/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  15\n","[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+  15\n","[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  16\n","[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  16\n","[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  20\n","[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  20\n","[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  20\n","[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  20\n","[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  20\n","[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  20\n","[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+  22\n","[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  22\n","[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  23\n","[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  25\n","[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  25\n","[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  25\n","[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  25\n","[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  25\n","[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  25\n","[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  26\n","[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  26\n","[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  28\n","[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  30\n","[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  30\n","[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  30\n","[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  30\n","[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  30\n","[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  30\n","[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  30\n","[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  32\n","[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  32\n","[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  34\n","[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  35\n","[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  35\n","[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  35\n","[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  35\n","[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  35\n","[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  36\n","[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  36\n","[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  37\n","[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  38\n","[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  39\n","[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  39\n","[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  39\n","[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  40\n","[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  40\n","[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  40\n","[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  41\n","[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  42\n","[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  43\n","[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  45\n","[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  45\n","[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  45\n","[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  45\n","[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  45\n","[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  45\n","[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  46\n","[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  50\n","[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  54\n","[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  55\n","[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  55\n","[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  55\n","[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  55\n","[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  55\n","[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  55\n","[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  57\n","[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  57\n","[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  60\n","[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  60\n","[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  60\n","[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  60\n","[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  64\n","[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  64\n","[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  64\n","[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  65\n","[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  66\n","[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  66\n","[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  68\n","[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  68\n","[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  68\n","[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  68\n","[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  69\n","[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  70\n","[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  70\n","[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  71\n","[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  72\n","[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  75\n","[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  75\n","[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  75\n","[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  75\n","[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  75\n","[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  76\n","[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  76\n","[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  77\n","[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  80\n","[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  81\n","[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  81\n","[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  81\n","[102/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  84\n","[103/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  84\n","[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  85\n","[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  85\n","[106/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  85\n","[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  86\n","[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  90\n","[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  90\n","[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  90\n","[111/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  90\n","[112/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  90\n","[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  90\n","[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  91\n","[115/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  93\n","[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  95\n","[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  97\n","[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  97\n","[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  97\n","[120/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  98\n","[121/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  98\n","[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  98\n","[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  98\n","[124/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 100\n","[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 100\n","[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+ 101\n","[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+ 101\n","[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 101\n","[129/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 101\n","[130/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 101\n","[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 102\n","[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 102\n","[133/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 103\n","[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 103\n","[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+ 105\n","[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+ 105\n","[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 105\n","[138/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 105\n","[139/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 105\n","[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 106\n","[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 107\n","[142/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 107\n","[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 110\n","[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+ 111\n","[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+ 111\n","[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 111\n","[147/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 114\n","[148/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 114\n","[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 114\n","[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 115\n","[151/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 115\n","[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 116\n","[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+ 117\n","[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+ 117\n","[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 117\n","[156/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 120\n","[157/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 120\n","[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 120\n","[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 120\n","[160/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 121\n","[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 122\n","[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+ 122\n","[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+ 122\n","[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 122\n","[165/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 125\n","[166/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 125\n","[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 125\n","[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 125\n","[169/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 126\n","[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 127\n","[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+ 127\n","[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 127\n","[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 127\n","[174/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 127\n","[175/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 127\n","[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 128\n","[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 128\n","[178/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 129\n","[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 130\n","[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 131\n","[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 131\n","[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 131\n","[183/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 131\n","[184/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 131\n","[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 131\n","[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 133\n","[187/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 133\n","[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 135\n","[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 140\n","[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 140\n","[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 140\n","[192/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 140\n","[193/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 140\n","[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 141\n","[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 141\n","[196/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 142\n","[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 143\n","[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 144\n","[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 144\n","[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 144\n","[201/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 144\n","[202/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 144\n","[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 144\n","[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 145\n","[205/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 146\n","[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 146\n","[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 150\n","[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 150\n","[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 150\n","[210/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 150\n","[211/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 150\n","[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 150\n","[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 151\n","[214/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 155\n","[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 156\n","[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 156\n","[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 156\n","[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 156\n","[219/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 156\n","[220/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 156\n","[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 156\n","[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 157\n","[223/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 158\n","[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 159\n","[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 160\n","[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 160\n","[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 160\n","[228/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 160\n","[229/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 160\n","[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 161\n","[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 162\n","[232/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 162\n","[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 165\n","[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 166\n","[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 166\n","[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 166\n","[237/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 170\n","[238/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 170\n","[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 170\n","[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 170\n","[241/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 171\n","[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 172\n","[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 172\n","[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 172\n","[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 172\n","[246/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 172\n","[247/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 172\n","[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 172\n","[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 173\n","[250/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 174\n","[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 175\n","[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 175\n","[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 175\n","[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 175\n","[255/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 175\n","[256/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 175\n","[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 176\n","[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 177\n","[259/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 178\n","[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 180\n","[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 181\n","[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 181\n","[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 181\n","[264/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 181\n","[265/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 181\n","[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 182\n","[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 182\n","[268/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 183\n","[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 184\n","[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 185\n","[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 185\n","[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 185\n","[273/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 185\n","[274/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 185\n","[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 186\n","[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 187\n","[277/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 188\n","[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 189\n","[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 190\n","[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 190\n","[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 190\n","[282/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+ 190\n","[283/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+ 190\n","[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 190\n","[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+ 192\n","[286/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+ 192\n","[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+ 193\n","[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 195\n","[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 195\n","[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 195\n","[291/291] Writing tensor output.weight                          | size 128256 x   4096  | type F16  | T+ 200\n","Wrote model-unsloth.F16.gguf\n","Unsloth: Conversion completed! Output location: ./model-unsloth.F16.gguf\n","Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n","main: build = 2769 (8843a98c)\n","main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n","main: quantizing './model-unsloth.F16.gguf' to './model-unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n","llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./model-unsloth.F16.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = .\n","llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv  12:                          general.file_type u32              = 1\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n","llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type  f16:  226 tensors\n","[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n","[   2/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[   3/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[   4/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[   6/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[   7/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[   8/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[   9/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  10/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  11/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  12/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  13/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[  14/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  15/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  16/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  17/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[  18/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  19/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  21/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[  23/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  24/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  25/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  26/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[  27/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  28/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  29/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  30/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  31/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[  32/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  33/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  34/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  35/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[  36/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  37/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  38/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  40/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  41/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  42/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  43/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  44/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  45/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  46/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  47/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  48/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  49/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  50/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  51/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  52/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  53/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  54/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  55/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  56/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  58/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[  59/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  60/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  61/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  62/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[  63/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  64/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  65/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  67/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  68/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  69/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  70/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  71/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  72/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  73/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  74/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  75/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  77/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  78/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  79/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  80/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  81/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  82/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  83/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  84/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  85/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[  86/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  87/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  88/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  89/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[  90/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  91/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[  92/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  93/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  94/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  95/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[  96/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  97/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  98/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[  99/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 101/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 103/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 104/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 105/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 106/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 107/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 108/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 110/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 111/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 113/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 114/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 115/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 116/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 117/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 118/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 119/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 120/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 121/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 122/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 123/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 124/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 125/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 126/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 127/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 128/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 129/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 130/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 131/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 132/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 133/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 134/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 135/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 136/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 137/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 139/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 140/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 141/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 142/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 143/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 144/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 146/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 148/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 149/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 150/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 151/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 152/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 153/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 154/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 155/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 156/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 158/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 159/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 160/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 161/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 162/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 163/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 164/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 165/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 166/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 167/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 168/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 169/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 170/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 171/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 172/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 173/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 174/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 175/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 176/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 177/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 178/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 179/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 180/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 181/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 182/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 183/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 184/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 185/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 186/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 187/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 189/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 191/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 192/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 193/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 194/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 195/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 196/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 197/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 198/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 199/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 200/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 202/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 203/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 204/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 205/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 207/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 208/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 209/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 210/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 212/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 213/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 214/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 215/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 216/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 217/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 218/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 220/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 222/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 223/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 224/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 225/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 227/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 228/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 229/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 230/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 231/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 232/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 233/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 234/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 235/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 236/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 237/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 238/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 239/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 240/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 241/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 242/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 243/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 244/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 245/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 246/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 247/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 248/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 249/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 250/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 251/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 252/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 253/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 254/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 256/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 257/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 258/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 259/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 260/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 261/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 262/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 263/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 264/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 266/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 267/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 268/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 269/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 270/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 271/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 272/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 273/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 274/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 275/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 276/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 277/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 278/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 279/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 280/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 281/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 283/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n","[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n","[ 285/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 286/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n","[ 287/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n","[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n","[ 291/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n","llama_model_quantize_internal: model size  = 15317.02 MB\n","llama_model_quantize_internal: quant size  =  4685.30 MB\n","Unsloth: Conversion completed! Output location: ./model-unsloth.Q4_K_M.gguf\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"5wboM9FVAj72"}},{"cell_type":"code","source":["#10挂载google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXLLnXwc11QF","executionInfo":{"status":"ok","timestamp":1714463900389,"user_tz":-480,"elapsed":32839,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"dcad8dec-7bff-4a4c-a9ba-819fa9ca4f09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkvvYuEoBIfT","executionInfo":{"status":"ok","timestamp":1714464017790,"user_tz":-480,"elapsed":363,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"e7a9e647-c82d-4e66-c6cb-46641227e2d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["\n","!ls -h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERIXOR6aBn1t","executionInfo":{"status":"ok","timestamp":1714464166294,"user_tz":-480,"elapsed":398,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"ea4a7e08-3277-4aea-ce96-29a2ddf548a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive\t\t\t      lora_model\t      model-unsloth.Q4_K_M.gguf\n","huggingface_tokenizers_cache  model\t\t      outputs\n","llama.cpp\t\t      model-unsloth.F16.gguf  sample_data\n"]}]},{"cell_type":"code","source":["#11复制模型到google drive\n","import shutil\n","source_file = '/content/model-unsloth.Q4_K_M.gguf'\n","destination_dir = '/content/drive/MyDrive'\n","destination_file = f'{destination_dir}/model-unsloth.Q4_K_M.gguf'\n","shutil.copy(source_file, destination_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"WPEfqzLeAn_e","executionInfo":{"status":"ok","timestamp":1714464155294,"user_tz":-480,"elapsed":62087,"user":{"displayName":"一繁李","userId":"17771601373040192001"}},"outputId":"a91262ae-058d-4f4b-ddbc-79d1412ffe19"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/model-unsloth.Q4_K_M.gguf'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[],"metadata":{"id":"fmAVEobJA3NC"},"execution_count":null,"outputs":[]}]}